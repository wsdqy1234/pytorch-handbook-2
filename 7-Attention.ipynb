{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Attention机制\n",
    "本节旨在介绍[Attention机制](https://zhuanlan.zhihu.com/p/46313756/)与两种典型的应用（Self-attention, Multi-head attention），及[基本的Attention实现方法](https://medium.com/intel-student-ambassadors/implementing-attention-models-in-pytorch-f947034b3e66)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Attention\n",
    "以NLP领域为例，常规的encoding是无法体现对一个句子序列中不同语素的关注程度的，然而一个句子中不同部分具有不同含义，并在意义上具有不同的重要性。\n",
    "\n",
    "Attention机制是一种能让模型对重要信息重点关注并充分吸收的技术，能够作用于任何序列模型中。其通过赋予序列中不同语素以不同权重，结合实际场景的优化目标（如情感分析将着重关注Like/Dislike这种语素），来实现对不同语素进行不同侧重的目标。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1.1 Attention机制流程\n",
    "**下面以seq2seq模型为例，阐述attention最基本的流程：**\n",
    "\n",
    "对于一个包含有n个单词的句子序列source $S=[w_1, w_2, \\cdots, w_n]$\n",
    "1. 应用某种方法将 $S$ 的每个单词 $w_i$ 编码为一个单独的向量 $v_i$；\n",
    "<p align=center>\n",
    "<img src=\"./fig/7-3.png\" width=700>\n",
    "</p>\n",
    "\n",
    "2. decoding阶段，使用学习到的Attention权重 $a_i$ 对1中得到的所有向量做线性加权 $\\sum_i a_iv_i$。\n",
    "<p align=center>\n",
    "<img src=\"./fig/7-4.png\" width=700>\n",
    "</p>\n",
    "\n",
    "3. 在decoder进行下一个单词的预测时，使用2中得到的线性组合。\n",
    "<p align=center>\n",
    "<img src=\"./fig/7-5.png\" width=700>\n",
    "</p>\n",
    "\n",
    "\n",
    "由此可以抽象出Attention实现的三要素，Query，Key，Value，其中Q与K用于计算线性权重，V用于加权\n",
    "\n",
    "<p align=center>\n",
    "<img src=\"./fig/7-1.png\" width=700>\n",
    "</p>\n",
    "\n",
    "对于Q, K, V的例子理解：\n",
    "\n",
    "<p align=center>\n",
    "<img src=\"./fig/7-2.png\" width=700>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1.2 Attention的核心-注意力权重计算\n",
    "Attention机制的核心在于如何通过Query和Key计算注意力权重，下面总结常用的几个方法：\n",
    "\n",
    "1. 多层感知机(Multi-Layer Perception, MLP)\n",
    "$$ a(q,k) = w_2^T tanh(W_1 [q;k])$$\n",
    "首先将向量$q$与$k$进行拼接，经过全连接$W_1$线性映射后，$tanh$激活，通过一个全连接$w_2$线性映射至一个值。\n",
    "\n",
    "MLP方法训练成本高，对大规模数据较为有效。\n",
    "\n",
    "2. Bilinear\n",
    "$$ a(q,k) = q^TWk$$\n",
    "通过一个权重矩阵$W$建立$q$与$k$之间的相关关系，简单直接，计算速度快。\n",
    "\n",
    "3. Dot Product\n",
    "$$ a(q,k) = q^Tk$$\n",
    "直接建立$q$与$k$之间的相关关系（内积，相似度），要求二者维度相同。\n",
    "\n",
    "4. Scaled-dot Product\n",
    "对3的改进，由于q和k的维度增加，会使得最后得到的内积a可能也会变得很大，这使得后续归一化softmax的梯度会非常小，不利于模型训练。参考[为什么dot-product需要被scaled](https://blog.csdn.net/qq_37430422/article/details/105042303)\n",
    "$$ a(q,k) = \\frac{q^Tk}{\\sqrt{d_k}}$$\n",
    "通过k的维度对a的尺度进行scaled，避免梯度消失问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Self-attention\n",
    "Self-attention是attention机制的一种应用，其中，attention完成了输入source和输出target之间的加权映射。而self-attention字如其名，通过使得source=target，自己对自己本身进行注意力机制计算，来捕获序列数据自身的相互依赖特性。\n",
    "\n",
    "即，在一般任务的Encoder-Decoder框架中，输入Source和输出Target内容是不一样的，比如对于英-中机器翻译来说，Source是英文句子，Target是对应的翻译出的中文句子，Attention机制发生在Target的元素Query和Source中的所有元素之间。\n",
    "\n",
    "而Self-attention的注意力机制，是在Source=Target的特殊情况下，内部元素之间的attention机制，其具体计算过程是一样的，只是计算对象发生了变化而已。\n",
    "\n",
    "<p align=center>\n",
    "<img src=\"./fig/7-6.png\" width=700>\n",
    "</p>\n",
    "\n",
    "如上图所示，我们将句子做self-attention，可以看到source中的语素'its'的attention集中在target中的语素'Law'与'application'上，这种self-attention使我们能够捕获这个句子内部不同元素间的依赖关系。\n",
    "\n",
    "很明显，引入self-attention后，序列数据中长距离的相互依赖性将更容易被捕获。对于RNN来说，依次序列计算难以捕获远距离的依赖性，但self-attention通过直接将序列数据中任意两个样本的联系通过一个计算步骤直接联系起来，极大地缩短了远距离依赖性的距离，有利于有效地利用这些远距离相互依赖的特征。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 Multi-head attention\n",
    "另一种有效的应用为multi-head attention,其主要思想为从**多视角**看待（Q, K, V）的attention映射关系，是attention的拓展版本。\n",
    "\n",
    "Multi-head attention通过设计h种不同的权重矩阵对 $(W_i^Q, W_i^K, W_i^V)_{i=1}^h$ 对 $ (Q, K, V)$进行attention计算，得到h个不同的 ${head_i}_{i=1}^h$ ，而后concat起来做一个全连接 $W^o$得到最后的attention输出，如图所示：\n",
    "\n",
    "<p align=center>\n",
    "<img src=\"./fig/7-7.png\" width=300>\n",
    "</p>\n",
    "\n",
    "$$head_i = attention(QW^Q, KW^K, VW^V)$$\n",
    "$$output = multihead(Q, K, V) = [head_1, \\cdots, head_h]W^o$$\n",
    "\n",
    "关于全连接 $(W_i^Q, W_i^K, W_i^V)$ 的输出维度 $(d^q, d^k, d^v)$，通常小于 $ (Q, K, V)$ 的输入维度 $d$，因为multi-head的计算成本过高，维度的增加将大大增加算法计算量，一般采用：\n",
    "\n",
    "$$d^q=d^k=d^v=d/h$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4 Attention机制的Torch实现\n",
    "关于各种Attention的实现可以多加利用[github的轮子](https://github.com/xmu-xiaoma666/External-Attention-pytorch)，不得不说Github永远的神！\n",
    "\n",
    "这里自己造个[简单的轮子](https://github.com/sooftware/attentions)，复现下Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4.1 Scaled Dot-Product Attention\n",
    "Attention组件示意图：\n",
    "\n",
    "<p align=center>\n",
    "<img src=\"./fig/7-8.png\" width=300>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from typing import Optional\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Args: dim\n",
    "        - dim (int): dimension of attention (commonly, d_k).\n",
    "\n",
    "    Inputs: query, key, value, mask\n",
    "        - **query** (batch, q_len, d_model): tensor containing projection vector for decoder. d_model -> dimension of model (feature)\n",
    "        - **key** (batch, k_len, d_model): tensor containing projection vector for encoder.\n",
    "        - **value** (batch, v_len, d_model): tensor containing features of the encoded input sequence.\n",
    "        - **mask** (batch, q_len, k_len): tensor containing indices to be masked.\n",
    "    \n",
    "    Outputs: context, attn\n",
    "        - **context**: tensor containing the context vector from mechanism.\n",
    "        - **attn**: tensor containing the attention from the encoder outputs.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim:int):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "        self.sqrt_dim = np.sqrt(dim)\n",
    "    \n",
    "    def forward(self, query:Tensor, key:Tensor, value:Tensor, mask: Optional[Tensor] = None):\n",
    "        # MatMul\n",
    "        score = torch.bmm(query, key.transpose(1, 2)) # (batch, q_len, k_len)\n",
    "        # Scale\n",
    "        score = score / self.sqrt_dim\n",
    "        # Mask (Opt)\n",
    "        if mask is not None:\n",
    "            score.masked_fill_(mask.view(score.size()), -float('Inf'))\n",
    "        # Softmax\n",
    "        attn = F.softmax(score, -1) # softmax along dimension \"k_len\"\n",
    "        # MatMul\n",
    "        context = torch.bmm(attn, value) # (batch, q_len, d_model)\n",
    "\n",
    "        return context, attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4.2 Multi-head Attention\n",
    "Multi-head attention的组件示意图：\n",
    "\n",
    "<p align=center>\n",
    "<img src=\"./fig/7-9.png\" width=300>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Project (q, k, v) h times with different, learned linear projections to d_head dimensions.\n",
    "\n",
    "    Args: d_model, num_heads\n",
    "        - d_model (int): The dimension of model (feature)\n",
    "        - num_heads (int): The number of attention heads.\n",
    "    \n",
    "    Inputs: query, key, value, mask\n",
    "        - **query** (batch, q_len, d_model)\n",
    "        - **key** (batch, k_len, d_model)\n",
    "        - **value** (batch, v_len, d_model)\n",
    "        - **mask** (batch, q_len, k_len): tensor containing indices to be masked.\n",
    "\n",
    "    Outputs: output, attn\n",
    "        - **output** (batch, q_len, d_model): tensor containing the output features\n",
    "        - **attn** (batch * num_heads, v_len): tensor containing the multi-head attention from the encoder outputs.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model:int = 512, num_heads:int = 8):\n",
    "        super(MultiHeadAttention,self).__init__()\n",
    "        \n",
    "        # Since d^q = d^k = d^v = d/h, d should be divided totally by h\n",
    "        assert d_model % num_heads == 0, \"Error: d_model % num_heads should be zero.\"\n",
    "        self.d_head = int(d_model / num_heads)\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # Instantiate a scaled dot-product attention object\n",
    "        self.scaled_dot_attn = ScaledDotProductAttention(self.d_head)\n",
    "\n",
    "        # Linear projection\n",
    "        self.query_proj = nn.Linear(d_model, self.d_head * num_heads) # 'H' Linear Layers\n",
    "        self.key_proj = nn.Linear(d_model, self.d_head * num_heads)\n",
    "        self.value_proj = nn.Linear(d_model, self.d_head * num_heads)\n",
    "\n",
    "        # Linear\n",
    "        self.Linear = nn.Linear(num_heads * self.d_head, d_model)\n",
    "    \n",
    "    def forward(self, query:Tensor, key:Tensor, value:Tensor, mask:Optional[Tensor] = None):\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        # Linear projection\n",
    "        query = self.query_proj(query).view(batch_size, -1, self.num_heads, self.d_head) # (batch, q_len, num_heads, d_head)\n",
    "        key = self.key_proj(key).view(batch_size, -1, self.num_heads, self.d_head)\n",
    "        value = self.value_proj(value).view(batch_size, -1, self.num_heads, self.d_head)\n",
    "        \n",
    "        # Mask [Optional]\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1).repeat(1, self.num_heads, 1, 1) # BxHxQ_lenxK_len\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        query = query.permute(2, 0, 1, 3).contiguous().view(batch_size * self.num_heads, -1, self.d_head) # (BxH, q_len, d_head)\n",
    "        key = key.permute(2, 0, 1, 3).contiguous().view(batch_size * self.num_heads, -1, self.d_head) # (BxH, q_len, d_head)\n",
    "        value = value.permute(2, 0, 1, 3).contiguous().view(batch_size * self.num_heads, -1, self.d_head) # (BxH, q_len, d_head)\n",
    "        context, attn = self.scaled_dot_attn(query, key, value, mask)\n",
    "\n",
    "        # Post-processing\n",
    "        context = context.view(self.num_heads, batch_size, -1, self.d_head)\n",
    "        context = context.permute(1, 2, 0, 3).contiguous().view(batch_size, -1, self.num_heads * self.d_head) # (B, q_len, Hxd_head)\n",
    "\n",
    "        # Linear\n",
    "        context = self.Linear(context)\n",
    "\n",
    "        return context, attn\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 附录：关于squeeze/unsqueeze, Tensor.view的一些说明\n",
    "1. squeeze为压缩操作（降维），unsqueeze为升维操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: torch.Size([1, 3, 4])\n",
      "Unsqueeze: torch.Size([1, 1, 3, 4])\n",
      "Squeeze: torch.Size([1, 3, 4])\n",
      "Squeeze: torch.Size([3, 4])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn([1, 3, 4])\n",
    "print('Before:', x.shape)\n",
    "y = x.unsqueeze(0) # 在索引0处升维\n",
    "print('Unsqueeze:', y.shape)\n",
    "\n",
    "z = y.squeeze(0) # 在索引0处降维\n",
    "print('Squeeze:', z.shape)\n",
    "\n",
    "h = z.squeeze(0) # 在索引0处降维\n",
    "print('Squeeze:', h.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然而，对于squeeze而言，其压缩操作只有当索引对应维度为1时才能生效，否则将不会做降维处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: torch.Size([2, 3])\n",
      "y: torch.Size([2, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn([1, 2, 3])\n",
    "y = torch.randn([2, 2, 3])\n",
    "\n",
    "x = x.squeeze(0)\n",
    "print('x:', x.shape)\n",
    "\n",
    "y = y.squeeze(0)\n",
    "print('y:', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 对于Tensor.view()，当view的维度与Tensor的维度不一致的时候，将按照Tensor的元素顺序和view的维度进行重新切割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 2.],\n",
      "         [4., 2.],\n",
      "         [3., 4.]]])\n",
      "tensor([[1., 2., 4.],\n",
      "        [2., 3., 4.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.Tensor([[[1,2,4], [2,3,4]]]) # Sequence: 1-2-4-2-3-4\n",
    "a.shape\n",
    "b = a.view([1,3,2]) # Now 1-2|4-2|3-4\n",
    "print(b)\n",
    "c = a.view([2,3]) # Now 1-2-4|2-3-4\n",
    "print(c)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('MAC-GPU')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "82ebbbba6002e52ffd5c3db618d60685134ba8ae181eb7df3dc9e0c9fc17132c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
