{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Basic Loss Function\n",
    "本节旨在介绍[不同任务下基础的Loss函数](https://zhuanlan.zhihu.com/p/377799012)，并结合例子进行说明。\n",
    "\n",
    "Loss Function 用于衡量模型输出与ground truth之间的误差，Loss越大意味着模型训练效果越差。\n",
    "\n",
    "在训练模型时，我们通常采用gradient descent来最小化loss function，直到loss收敛。\n",
    "\n",
    "此外，[PyTorch的其他损失函数](https://zhuanlan.zhihu.com/p/61379965)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1 Regression Loss （回归损失）\n",
    "回归损失通常针对连续型变量计算损失，常包含 **MAE (L1 Loss), MSE (L2 Loss)**。\n",
    "\n",
    "### 9.1.1 Mean Absolute Loss (MAE) - L1 Loss\n",
    " MAE又称为L1 Loss，因为其与L1范数相似，都是计算样本间差的绝对值。\n",
    "\n",
    "对于 模型预测值 $\\hat{\\bm{y}}=\\{\\hat{y}_i\\}$ 与 样本真实值 $\\bm{y}=\\{y_i\\}$，MAE计算其所有元素之差的绝对值，并返回其平均值：\n",
    "\n",
    "$$MAE = \\frac{1}{n}\\sum_{i=1}^n|\\hat{y}_i-y_i|$$\n",
    "\n",
    "优势：稀疏性强，对离群点具有鲁棒性（因为离群点的梯度不会被MAE放大，使模型不会为了少数异常值而进行大范围的调整，牺牲其他正常样本）\n",
    "\n",
    "缺陷：在原点处导数不连续，不同损失值的梯度一样大，使其梯度求解效率低下，导致收敛速度慢，不利于网络的学习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "output = torch.tensor([2, 3, 4], dtype = torch.float)\n",
    "target = torch.tensor([3, 4, 5], dtype = torch.float)\n",
    "\n",
    "# MAE Loss (Self-implemented)\n",
    "def MAE_loss(output, target):\n",
    "    diff = torch.abs(output - target)\n",
    "    loss = torch.sum(diff)/diff.size(0)\n",
    "    return loss\n",
    "\n",
    "print(MAE_loss(output, target))\n",
    "\n",
    "# Torch MAE Loss (L1 Loss)\n",
    "loss = nn.L1Loss()\n",
    "MAE = loss(output, target) # (|3-2|+|4-3|+|5-4|)/3 = 1\n",
    "print(MAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1.2 Mean Squared Loss (MSE) - L2 Loss\n",
    "MSE又称为L2 Loss，因为其与L2范数相似，都是计算样本间差的平方\n",
    "\n",
    "对于 模型预测值 $\\hat{\\bm{y}}=\\{\\hat{y}_i\\}$ 与 样本真实值 $\\bm{y}=\\{y_i\\}$，MSE计算其所有元素之差的平方，并返回其平均值：\n",
    "\n",
    "$$MSE = \\frac{1}{n}\\sum_{i=1}^n(\\hat{y}_i-y_i)^2$$\n",
    "\n",
    "优势：收敛速度快（梯度对大差值有放大效果），易于训练\n",
    "\n",
    "缺陷：对异常值十分敏感，因为其梯度更新的方向容易受到离群点主导"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# MSE Loss (Self-implemented)\n",
    "def MSE_loss(output, target):\n",
    "    diff = output-target\n",
    "    diff = diff.pow(2)\n",
    "    loss = torch.sum(diff)/diff.size(0)\n",
    "    return loss\n",
    "\n",
    "print(MSE_loss(output, target))\n",
    "\n",
    "# Torch MSE Loss\n",
    "loss = nn.MSELoss()\n",
    "MSE = loss(output, target)\n",
    "print(MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2 Classification Loss（分类损失）\n",
    "分类损失通常针对离散型变量计算损失，常包含KL散度损失（KL Divergence Loss）、交叉熵损失（Cross Entropy Loss）。\n",
    "\n",
    "### 9.2.1 KL Divergence Loss\n",
    "KL散度损失用于刻画两个概率分布间的差异程度，若两个分布差异较小，则KL散度趋向于0，否则KL散度将很大。KL散度定义为\n",
    "\n",
    "<p align=center>\n",
    "<img src=\"./fig/9-0.png\" width=800>\n",
    "</p>\n",
    "\n",
    "通过KL散度可以衡量预测分类概率分布q（视为一个多项分布）与真实分类概率分布p间的差异，从而构建KL散度损失。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1185, 0.8756, 0.0059])\n",
      "tensor(0.7032)\n",
      "tensor(0.7032)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fw/by0ngc4j67x7597xgdhn13xr0000gn/T/ipykernel_44932/3173961560.py:2: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  predicted_softmax = F.softmax(predicted)\n"
     ]
    }
   ],
   "source": [
    "predicted = torch.tensor([5, 7, 2], dtype=torch.float)\n",
    "predicted_softmax = F.softmax(predicted)\n",
    "print(predicted_softmax)\n",
    "ground_truth = torch.tensor([0.3, 0.5, 0.2], dtype=torch.float)\n",
    "\n",
    "# KL Divergence Loss (Self divergence)\n",
    "def KL_loss(predicted, ground_truth):\n",
    "    tmp = torch.div(ground_truth, predicted)\n",
    "    tmp = tmp.log()\n",
    "    KL_loss = torch.sum(ground_truth.matmul(tmp))\n",
    "    return KL_loss\n",
    "\n",
    "print(KL_loss(predicted_softmax, ground_truth))\n",
    "\n",
    "# Torch KL Div Loss\n",
    "loss = nn.KLDivLoss(reduction=\"sum\")\n",
    "KLD_loss = loss(predicted_softmax.log(), ground_truth) # The input must be log(q)\n",
    "print(KLD_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 9.2.2 Cross Entropy Loss\n",
    "交叉熵损失起源于信息论，从信息量->熵->交叉熵。\n",
    "\n",
    "最早对信息量的定义为：若一件事情发生的概率很小，但其发生了，则该事件发生所包含的信息量将很大；若一件事情发生的概率很大，其发生了，则该事件发生所包含的信息量将很小。\n",
    "\n",
    "<p align=center>\n",
    "<img src=\"./fig/9-1.png\" width=800>\n",
    "</p>\n",
    "\n",
    "如果将事件X的所有可能性列出来，就可以求得该事件信息量的期望（熵），因此熵定义为：\n",
    "\n",
    "<p align=center>\n",
    "<img src=\"./fig/9-2.png\" width=700>\n",
    "</p>\n",
    "\n",
    "对于上一小节中阐述的用以衡量两个概率分布差异的KL散度（相对熵），将log拆开，可知真正衡量p与q分布间关系的为其中一项：\n",
    "\n",
    "<p align=center>\n",
    "<img src=\"./fig/9-3.png\" width=700>\n",
    "</p>\n",
    "\n",
    "因此，类似于互相关性的定义，将这一项定义为交叉熵。同样地，通过交叉熵，我们也可以衡量两个分布间的差异性，若交叉熵较大，说明分布间差异较大。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.7328)\n",
      "tensor(1.7328)\n"
     ]
    }
   ],
   "source": [
    "# Cross Entropy Loss (Self-implemented)\n",
    "def croEntLoss(predicted, ground_truth):\n",
    "    loss = -ground_truth.matmul(predicted.log()).sum()\n",
    "    return loss\n",
    "\n",
    "print(croEntLoss(predicted_softmax, ground_truth))\n",
    "\n",
    "# Torch Cross Entropy Loss\n",
    "loss = nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "CEL = loss(predicted_softmax.log(), ground_truth) # The input must be log(q)\n",
    "print(CEL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('MAC-GPU')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "82ebbbba6002e52ffd5c3db618d60685134ba8ae181eb7df3dc9e0c9fc17132c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
